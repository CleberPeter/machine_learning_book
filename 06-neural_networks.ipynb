{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais\n",
    "\n",
    "As Redes Neurais Artificiais foram propostas inicialmente em meados de 1980 com o objetivo de imitar a forma como o cérebro humano aprende, especialmente no que se refere as conexões entre os neurônios. \n",
    "\n",
    "A ideia surgiu da observação de como partes do cerébro fisicamente vinculadas a um determinado sentido (audição, visão, tato) podem aprender a responder aos outros. Um dos experimentos realizados consistiu em redirecionar os sinais elétricos do nervo óptico (visão) para a região do cérebro responsável pela audição, o resultado surpreendente foi que, após algum tempo, a cobaia \"reaprendeu\" a enxergar. \n",
    "\n",
    "Com a adaptatividade do cérebro humano em mente o objetivo dos autores do modelo de rede neural foi o de propor uma abordagem de aprendizado aplicável a qualquer tipo de problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivação\n",
    "\n",
    "A necessidade de um novo algoritmo para aprendizado de máquina surge de problemas não lineares com uma grande quantidade de entradas, $n > 100 $. Verificamos anteriormente que tanto a regressão linear quanto a logística podem ser adaptadas para mapearem superfícies não lineares, porém, para tanto é necessário adicionar novas entradas que representem a parcela quadrática, cúbica, ou de maior ordem conforme a superfície desejada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomando como exemplo os modelos linear e quadrático apresentados abaixo é possível verificar que a não linearidade aumenta substancialmente a quantidade de parâmetros do modelo.\n",
    "\n",
    "$$h_{\\theta}(x) = \\theta_{0} + \\theta_{1} x_1 + \\theta_{2} x_2$$\n",
    "$$h_{\\theta}(x) = \\theta_{0} + \\theta_{1} x_1 + \\theta_{2} x_2 + \\theta_{3} x_1 x_2 + \\theta_{4} x_1^2 + \\theta_{5} x_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a composição de um modelo não linear de um classificador de imagens com 50x50 pixels em escala de cinza ($n = 2500$) cada entrada deve ser combinada com as demais sem repetição, ou seja:\n",
    "\n",
    "$$h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1 x_2 + ... + \\theta x_1 x_{2500} + \\\\  \\theta x_2 + \\theta x_2^2 + \\theta x_2 x_3 + ... + \\theta x_2 x_{2500} + \\\\ ... + \\theta x_{2500} + \\theta x_{2500}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido a não repetição, $x_1$ realizará 2500 combinações, $x_2$ 2499, $x_3$ 2498 e assim por diante. Em outras palavras, a quantidade de parâmetros do modelo pode ser escrita na forma de uma soma de PA somada com os (n + 1) parâmetros já existentes no modelo linear:\n",
    "\n",
    "$$k = (2500 + 2499 + 2498 + ... + 3 + 2 + 1) + (1 + 2500)$$\n",
    "$$k = \\frac{(n+1) n}{2} + (n+1) = \\frac{(n+1) (n+ 2)}{2} = 3.128.751 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ademais, caso o classificador fosse projetado para processar imagens RGB ($n = 7500$) a quantidade de parâmetros necessária giraria em torno de 28 milhões. Enfim, a quantidade absurda de parâmetros, mesmo em um exemplo simples como o ilustrado, deixa clara a necessidade de uma nova estratégia para a composição de modelos não lineares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As redes neurais são formadas por camadas, onde cada camada possui uma determinada quantidade de nodos. A primeira camada, denominada de **entrada**, recebe $x$ e possui portanto $n$ nodos. A última camada, denominada de **saída**, fornece $h_\\Theta(x)$ e possui uma quantidade de nodos igual a quantidade de saídas do modelo. As demais camadas são denominadas de **ocultas** e apresentam uma quantidade arbitrária de nodos. \n",
    "\n",
    "A título de exemplo, a Figura 1 ilustra uma rede neural com **2 nodos na camada de entrada**, **1 camada oculta com 2 nodos** e **uma camada de saída com somente 1 nodo**.\n",
    "\n",
    "<img src=\"imgs/neural_network.png\" alt=\"linear_model\" width=\"400\" style=\"display:block; margin:auto\"/>\n",
    "\n",
    "<div align=\"center\">\n",
    "Rede Neural.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os nodos $a^{(j)}_i$ são identificados conforme sua posição na rede, de tal forma que $j$ representa o número da camada e $i$ a posição do nodo dentro da camada. Sendo assim, na rede exemplificada pela Figura acima os nodo $a^{(2)}_1$ e $a^{(2)}_2$ representam o primeiro e o segundo nodo da camada oculta, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagação para Frente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Propagação para Frente (Forward Propagation), utilizada para determinar $h_\\Theta(x)$, se vale do fato que nas redes neurais as saídas de cada camada são conectadas as entradas da camada imediatamente posterior. \n",
    "\n",
    "O método consiste primeiramente em inserir as entradas do modelo na primeira camada da rede e em seguida realizar de forma iterativa e em direção a saída a propagação dos dados. Na propagação a saída de cada camada é dada pela combinação linear das suas entradas, ponderadas pelos parâmetros da camada e aplicada em uma **função de ativação $g(z)$**. Em outras palavras:\n",
    "\n",
    "$$a^{(1)} = x$$\n",
    "$$a^{(k)} = g(\\Theta^{(k-1)} a^{(k-1)})$$\n",
    "\n",
    "A função de ativação empregada varia conforme o tipo de problema, entretanto, quase sempre são utilizadas para refletir a característica não linear dos dados. São alguns exemplos de funções de ativação: a função simóide, a tangente hiperbólica e a ReLu.\n",
    "\n",
    "Posto isto, podemos definir a saída da camada oculta presente na Figura: \n",
    "\n",
    "$$a_1^{(2)} = g(\\Theta_{11}^{(1)} x_1 + \\Theta_{12}^{(1)} x_2)$$\n",
    "$$a_2^{(2)} = g(\\Theta_{21}^{(1)} x_1 + \\Theta_{22}^{(1)} x_2)$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} a_1^{(2)} \\\\ a_2^{(2)} \\end{bmatrix} =   \n",
    "g(\\begin{bmatrix} \\Theta_{11}^{(1)} & \\Theta_{12}^{(1)} \\\\\n",
    "                \\Theta_{21}^{(1)} & \\Theta_{22}^{(1)}\n",
    "\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix})\n",
    "= g(\\Theta^{(1)} x)\n",
    "$$\n",
    "\n",
    "Após, podemos escrever a saída do modelo:\n",
    "\n",
    "$$\n",
    "h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{11}^{(2)} a_1^{(2)} + \\Theta_{12}^{(2)} a_2^{(2)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_\\Theta(x) = \\begin{bmatrix} a_1^{(3)} \\end{bmatrix} =\n",
    "g(\\begin{bmatrix} \\Theta_{11}^{(2)} & \\Theta_{12}^{(2)}\n",
    "\\end{bmatrix} \\begin{bmatrix} a_1^{(2)} \\\\ a_2^{(2)} \\end{bmatrix})\n",
    "= g(\\Theta^{(2)} a^{(2)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viés (Bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em redes neurais é comum a adição de um nodo de **viés** igual a $1$ em cada uma das camadas que, diferentemente dos demais, não é conectado a camada anterior. Este nodo, representado na Figura 1 pelos nodos $x_0$ e $a_0^{(2)}$, tem como próposito fornecer maior expressividade ao modelo.\n",
    "\n",
    "<img src=\"imgs/neural_network_with_bias.png\" alt=\"linear_model\" width=\"400\" style=\"display:block; margin:auto\"/>\n",
    "\n",
    "<div align=\"center\">\n",
    "Figura 1 - Rede Neural com Viés.\n",
    "</div>\n",
    "\n",
    "O impacto do viés pode ser verificado na resposta de uma rede neural com somente um nodo e uma função de ativação linear, ilustrada pela Figura abaixo.\n",
    "\n",
    "<img src=\"imgs/bias_impact_on_neural_network.png\" alt=\"linear_model\" width=\"400\" style=\"display:block; margin:auto\"/>\n",
    "\n",
    "<div align=\"center\">\n",
    "Impacto do Bias na Rede Neural.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sem o termo de viés o modelo consegue mapear somente dados que passam pela origem do plano cartesiano, enquanto que com o termo o grau de flexibilidade do modelo se torna maior. Abaixo é definido o modelo da rede neural com a presença de viés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$a_1^{(2)} = g(\\Theta_{11}^{(1)} x_0 + \\Theta_{12}^{(1)} x_1 + \\Theta_{13}^{(1)} x_2) = g(\\Theta_{11}^{(1)} + \\Theta_{12}^{(1)} x_1 + \\Theta_{13}^{(1)} x_2)$$\n",
    "$$a_2^{(2)} = g(\\Theta_{21}^{(1)} x_0 + \\Theta_{22}^{(1)} x_1 + \\Theta_{23}^{(1)} x_2) = g(\\Theta_{21}^{(1)} + \\Theta_{22}^{(1)} x_1 + \\Theta_{23}^{(1)} x_2)$$\n",
    "$$\n",
    "\n",
    "\\begin{bmatrix} a_1^{(2)} \\\\ a_2^{(2)} \\end{bmatrix} =  \n",
    "g(\\begin{bmatrix} \\Theta_{11}^{(1)} & \\Theta_{12}^{(1)} & \\Theta_{13}^{(1)} \\\\\n",
    "                \\Theta_{21}^{(1)} & \\Theta_{22}^{(1)} & \\Theta_{23}^{(1)}\n",
    "\\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix})\n",
    "= g(\\Theta^{(1)} x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{11}^{(2)} a_0^{(2)} + \\Theta_{12}^{(2)} a_1^{(2)} + \\Theta_{13}^{(2)} a_2^{(2)}) = g(\\Theta_{11}^{(2)} + \\Theta_{12}^{(2)} a_1^{(2)} + \\Theta_{13}^{(2)} a_2^{(2)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_\\Theta(x) = \\begin{bmatrix} a_1^{(3)} \\end{bmatrix} =\n",
    "g(\\begin{bmatrix} \\Theta_{11}^{(2)} & \\Theta_{12}^{(2)} & \\Theta_{13}^{(2)}\n",
    "\\end{bmatrix} \\begin{bmatrix} a_0^{(2)} \\\\ a_1^{(2)} \\\\ a_2^{(2)} \\end{bmatrix})\n",
    "= g(\\Theta^{(2)} a^{(2)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada camada possui sua própria matriz de parâmetros os quais, no contexto das redes neurais, também são chamados de **pesos**. $\\Theta^{(j)}_{r x s}$ reflete os parâmetros da camada $j$ em que $r$ é dado pela quantidade de nodos da próxima camada e $s$ pela quantidade de nodos da camada atual $+1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O script a seguir implementa a propagação para frente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 17 # makes the random numbers predictable\n",
    "\n",
    "def add_ones_column(x):\n",
    "    \"\"\"\n",
    "    Append ones on first column ([a b c] -> [1 a b c])\n",
    "    Arguments:\n",
    "        x: np.array (mxn)\n",
    "    Returns:\n",
    "        np.array (mxn+1)\n",
    "    \"\"\"\n",
    "    [m, n] = np.shape(x)\n",
    "    new_x = np.zeros([m, n+1])\n",
    "    new_x[:,0] = np.ones(m)\n",
    "    new_x[:,1:] = x[:, 0:]\n",
    "\n",
    "    return new_x\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Logistic function\n",
    "    Arguments:\n",
    "        z: np.array (mx1)\n",
    "    Returns:\n",
    "        np.array (mx1)\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def forward_propagation(x, o_list):\n",
    "    \"\"\"\n",
    "    Forward Propagation\n",
    "    Arguments:\n",
    "        x: np.array (mxn)\n",
    "        o_list: list of np.array (rxs). \n",
    "                First should be r = n+1 and last s = k \n",
    "    Returns:\n",
    "        list of np.array (mxs)\n",
    "    \"\"\"\n",
    "    a = [x]\n",
    "    a_l = add_ones_column(x) # bias\n",
    "    for o in o_list:\n",
    "        a_l = sigmoid(np.dot(a_l, o))\n",
    "        a.append(a_l)\n",
    "        a_l = add_ones_column(a_l) # bias\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a definição do modelo, uma observação se faz necessária. A última camada da rede neural possui o mesmo modelo da regressão logística, sendo assim, qual a vantagem deste tipo de abordagem ?\n",
    "\n",
    "A grande vantagem das redes neurais reside justamente na presença das camadas ocultas. O mapeamento de funções não lineares através da regressão logística era possível porém dependia da escolha adequada do polinômio, já para as redes neurais esse processo é automatizado pela fase de treinamento com a definição da matriz $\\Theta^1$.\n",
    "\n",
    "Ao invés de injetar as entradas diretamente no modelo de regressão logística, $\\Theta^1$ seleciona frações das entradas e as combina para permitir a composição de modelos altamente não lineares sem a necessidade de criação de novas entradas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador Binário\n",
    "\n",
    "A título de exemplo, a seguir é elaborada uma arquitetura de rede neural para classificação de dados binários segundo o operador lógico **XOR**.\n",
    "\n",
    "<img src=\"imgs/xor_data_distribution.png\" alt=\"linear_model\" width=\"400\" style=\"display:block; margin:auto\"/>\n",
    "\n",
    "<div align=\"center\">\n",
    "Distribuição de Dados do Operador XOR.\n",
    "</div>\n",
    "\n",
    "Uma forma de visualizar como modelos complexos podem ser obtidos com a adição de camadas ocultas é realizar a construção da rede de forma modular. Sendo assim, a rede neural do operado XOR é construída pelo agrupamento dos operadores **AND**, **OR** e **(NOT $x_1$) AND (NOT $x_2$)**, desenvolvidos a seguir.\n",
    "\n",
    "### Operadores AND, OR e (NOT $x_1$) AND (NOT $x_2$)\n",
    "\n",
    "Abaixo são mostrados os três operadores lógicos cuja construção é realizada com somente uma camada. Os parâmetros dos modelos são definidos de forma arbitrária e a função de ativação utilizada é a sigmóide.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"imgs/ann_and.png\" alt=\"Drawing\"/> </td>\n",
    "<td> <img src=\"imgs/ann_or.png\" alt=\"Drawing\"/> </td>\n",
    "<td> <img src=\"imgs/ann_not_x1_and_not_x2.png\" alt=\"Drawing\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "<div align=\"center\">\n",
    "Redes Neurais dos Operadores Lógicos.\n",
    "</div>\n",
    "\n",
    "### Operador XOR\n",
    "\n",
    "Enfim o operador XOR pode ser contruído com a agregação dos três operadores desenvolvidos anteriormente, arranjados conforme a Figura abaixo.\n",
    "\n",
    "<img src=\"imgs/ann_xor.png\" alt=\"linear_model\" width=\"600\" style=\"display:block; margin:auto\"/>\n",
    "\n",
    "<div align=\"center\">\n",
    "Rede Neural do Operador XOR.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme pode ser verificado na Figura acima é possível a criação de um modelo altamente não linear com a adição de somente uma camada oculta. O modelo obtido é definido a seguir:\n",
    "\n",
    "$$XOR = h_\\Theta(x) = (x_1 . x_2) | (\\neg{x_1} . \\neg{x_2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O script abaixo comprova o funcionamento do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99954561e-01]\n",
      " [4.54803785e-05]\n",
      " [4.54803785e-05]\n",
      " [9.99954561e-01]]\n"
     ]
    }
   ],
   "source": [
    "# XOR operator\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # 4x2\n",
    "\n",
    "o_list = []\n",
    "o_1 = np.array([[-30, 10], [20, -20], [20, -20]]) # 3x2\n",
    "o_list.append(o_1)\n",
    "o_2 = np.array([[-10], [20], [20]]) # 3x1\n",
    "o_list.append(o_2)\n",
    "\n",
    "print(forward_propagation(x, o_list)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma geral, as camadas ocultas atuam como uma espécie de abstração do modelo de tal forma que cada camada captura determinados aspectos dos dados. Baseado nos parâmetros definidos durante a fase de treinamento, as camadas se especializam na detecção de padrões e somente enviam a informação para a camada seguinte quando o padrão é evidenciado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador Multiclasse\n",
    "\n",
    "A composição de um classificador multiclasse através das redes neurais é realizada com a definição da camada de saída com uma quantidade de nodos igual a quantidade de classes previstas pelo problema, ou seja, $h_\\Theta(x) \\in {\\rm I\\!R}^k$ onde $k$ é igual ao número de classes.\n",
    "\n",
    "<img src=\"imgs/ann_multiclass_classifier.png\" alt=\"linear_model\" width=\"400\" style=\"display:block; margin:auto\"/>\n",
    "\n",
    "<div align=\"center\">\n",
    "Classificador Multiclasse com Redes Neurais.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Exemplo da figura acima a camada de saída possui $3$ nodos, ou seja, o problema prevê $3$ classes possíveis para os dados. Cada nodo de saída fornecerá a probabilidade dos dados de entrada estarem vinculados a sua classe. Em outras palavras:\n",
    "\n",
    "$$h_\\Theta(x) = \\begin{bmatrix} 0.1 & 0.2 & 0.7\\end{bmatrix}^T $$\n",
    "\n",
    "Indica 70% de chance dos dados de entrada pertencerem a um elemento da classe iris-virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função Custo\n",
    "\n",
    "A função custo de um modelo de rede neural para classificação multiclasse é uma expansão do modelo desenvolvido para o regressor logístico na estratégia *Um-Contra-Todos*. A diferença do modelo definido abaixo é a quantidade de saídas, afinal tanto $y$ quanto $h_\\Theta(x)$ $\\in {\\rm I\\!R}^c$ em que $c$ é igual ao número de classes do problema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\Theta) = \\frac{1}{m} \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{c} -y_k^{i} (log(h_{\\Theta}(x^{i})_k) - (1 - y_k^{i}) (log(1 - h_{\\Theta}(x^{i})_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo desta função custo é penalizar caso qualquer uma das $c$ saídas do modelo seja diferente do esperado. Desta forma perante a seguinte situação:  \n",
    "\n",
    "$$y^i = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}^T$$\n",
    "$$ h_{\\Theta}(x^{i}) = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}^T$$\n",
    "\n",
    "Haverá uma penalização dupla, visto $h_{\\Theta}(x^{i})_0$ deveria ser $0$ e $h_{\\Theta}(x^{i})_2$ deveria ser 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento\n",
    "\n",
    "A etapa de treinamento de um modelo de rede neural consiste em determinar os parâmetros $\\Theta$ que minimizam a função custo. Conforme já discutido, os algoritmos destinados a esta tarefa são os otimizadores que possuem como base comum o cálculo do gradiente. O gradiente, por sua vez, é dado pela derivada parcial do custo em relação aos parâmetros do modelo, entretanto, no caso das redes neurais cada camada possui sua própria matriz de parâmetros. \n",
    "\n",
    "Sendo assim, o gradiente será definido pela derivada parcial do custo em relação a cada um dos elementos da matriz $\\Theta^{(l)}$ em que $l$ representa o número da camada.\n",
    "\n",
    "$$\\nabla J(\\Theta) = \\frac{\\partial J(\\Theta)}{\\partial \\Theta^{(l)}_{ij}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A título de exemplo do cálculo do gradiente, será utilizada uma rede neural para classificação multiclasse com função de ativação sigmóide, 2 duas entradas, 1 camada oculta com 2 nodos e 2 saídas, mostrada na Figura abaixo:\n",
    "\n",
    "<img src=\"imgs/ann_multiclass_example.png\" alt=\"linear_model\" width=\"800\" style=\"display:block; margin:auto\"/>\n",
    "\n",
    "<div align=\"center\">\n",
    "Rede Neural com duas Saídas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente reescreveremos a função custo para denotar as duas saídas:\n",
    "\n",
    "$$J(\\Theta) = \\frac{1}{m} \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{c} -y_k^{i} (log(h_{\\Theta}(x^{i})_k) - (1 - y_k^{i}) (log(1 - h_{\\Theta}(x^{i})_k)$$\n",
    "\n",
    "$$h_\\Theta(x) = a^{3}$$\n",
    "\n",
    "$$J(\\Theta) = \\frac{-1}{m} \\sum\\limits_{i=1}^{m} y_1^{i} log(a_1^{3}) + (1 - y_1^{i}) (log(1 - a_1^{3})) + y_2^{i} log(a_2^{3}) + (1 - y_2^{i}) (log(1 - a_2^{3})$$\n",
    "\n",
    "A seguir calculamos a derivada parcial para os parâmetros da camada $\\Theta^{(2)}$: \n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\partial \\Theta^{(2)}_{11}} = \\frac{-1}{m} \\sum\\limits_{i=1}^{m} (\\frac{y_1^{i}}{a_1^{3}} \\frac{\\partial a_1^{3}}{\\partial \\Theta^{(2)}_{11}} + \\frac{1 - y_1^{i}}{1 - a_1^{3}} \\frac{\\partial (1 - a_1^{3})}{\\partial \\Theta^{(2)}_{11}}) + (\\frac{y_2^{i}}{a_2^{3}} \\frac{\\partial a_2^{3}}{\\partial \\Theta^{(2)}_{11}} + \\frac{1 - y_2^{i}}{1 - a_2^{3}} \\frac{\\partial (1 - a_2^{3})}{\\partial \\Theta^{(2)}_{11}})$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\partial \\Theta^{(2)}_{11}} = \\frac{-1}{m} \\sum\\limits_{i=1}^{m} (\\frac{y_1^{i}}{a_1^{3}} g(z_1^{3}) (1 - g(z_1^{3})) \\frac{\\partial (z_1^{3})}{\\partial \\Theta^{(2)}_{11}} - \\frac{1 - y_1^{i}}{1 - a_1^{3}} g(z_1^{3}) (1 - g(z_1^{3})) \\frac{\\partial (z_1^{3})}{\\partial \\Theta^{(2)}_{11}}) + \\\\\n",
    "\n",
    "(\\frac{y_2^{i}}{a_2^{3}} g(z_2^{3}) (1 - g(z_2^{3})) \\frac{\\partial (z_2^{3})}{\\partial \\Theta^{(2)}_{11}} - \\frac{1 - y_2^{i}}{1 - a_2^{3}} g(z_2^{3}) (1 - g(z_2^{3})) \\frac{\\partial (z_2^{3})}{\\partial \\Theta^{(2)}_{11}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porém $\\frac{\\partial (z_2^{3})}{\\partial \\Theta^{(2)}_{11}} = 0$, ou seja, a variação de $\\Theta^{(2)}_{11}$ não afeta a saída $a_2^{3}$. Sendo assim:\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\partial \\Theta^{(2)}_{11}} = \\frac{-1}{m} \\sum\\limits_{i=1}^{m} (\\frac{y_1^{i}}{a_1^{3}} g(z_1^{3}) (1 - g(z_1^{3})) \\frac{\\partial (z_1^{3})}{\\partial \\Theta^{(2)}_{11}} - \\frac{1 - y_1^{i}}{1 - a_1^{3}} g(z_1^{3}) (1 - g(z_1^{3})) \\frac{\\partial (z_1^{3})}{\\partial \\Theta^{(2)}_{11}})$$\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\partial \\Theta^{(2)}_{11}} = \\frac{-1}{m} \\sum\\limits_{i=1}^{m} \\frac{y_1^{i}}{a_1^{3}} g(z_1^{3}) (1 - g(z_1^{3})) a_1^{2} - \\frac{1 - y_1^{i}}{1 - a_1^{3}} g(z_1^{3}) (1 - g(z_1^{3})) a_1^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os demais parâmetros da camada podem ser determinados de forma análoga:\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\partial \\Theta^{(2)}_{12}} = \\frac{-1}{m} \\sum\\limits_{i=1}^{m} (\\frac{y_1^{i}}{a_1^{3}} g(z_1^{3}) (1 - g(z_1^{3})) a_2^{2} - \\frac{1 - y_1^{i}}{1 - a_1^{3}} g(z_1^{3}) (1 - g(z_1^{3})) a_2^{2}$$\n",
    "$$\\frac{\\partial J(\\Theta)}{\\partial \\Theta^{(2)}_{21}} = \\frac{-1}{m} \\sum\\limits_{i=1}^{m} (\\frac{y_2^{i}}{a_2^{3}} g(z_2^{3}) (1 - g(z_2^{3})) a_1^{2} - \\frac{1 - y_2^{i}}{1 - a_2^{3}} g(z_2^{3}) (1 - g(z_2^{3})) a_1^{2}$$\n",
    "$$\\frac{\\partial J(\\Theta)}{\\partial \\Theta^{(2)}_{22}} = \\frac{-1}{m} \\sum\\limits_{i=1}^{m} (\\frac{y_2^{i}}{a_2^{3}} g(z_2^{3}) (1 - g(z_2^{3})) a_2^{2} - \\frac{1 - y_2^{i}}{1 - a_2^{3}} g(z_2^{3}) (1 - g(z_2^{3})) a_2^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a regra da cadeia, definida abaixo, a notação pode ser severamente simplificada:\n",
    "\n",
    "$$\\frac{\\partial p(q(z))}{\\partial z} = \\frac{\\partial p}{\\partial q} \\frac{\\partial q}{\\partial z}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(2)}_{11}} = \\frac{\\partial J(\\Theta)}{a_1^{(3)}} \\frac{\\partial a_1^{(3)}}{z_1^{(3)}} \\frac{\\partial z_1^{(3)}}{\\Theta^{(2)}_{11}}$$\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(2)}_{12}} = \\frac{\\partial J(\\Theta)}{a_1^{(3)}} \\frac{\\partial a_1^{(3)}}{z_1^{(3)}} \\frac{\\partial z_1^{(3)}}{\\Theta^{(2)}_{12}}$$\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(2)}_{21}} = \\frac{\\partial J(\\Theta)}{a_2^{(3)}} \\frac{\\partial a_2^{(3)}}{z_2^{(3)}} \\frac{\\partial z_2^{(3)}}{\\Theta^{(2)}_{21}}$$\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(2)}_{22}} = \\frac{\\partial J(\\Theta)}{a_2^{(3)}} \\frac{\\partial a_2^{(3)}}{z_2^{(3)}} \\frac{\\partial z_2^{(3)}}{\\Theta^{(2)}_{22}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a determinação dos parâmetros da primeira camada é importante observar que a matriz $\\Theta^{(1)}$ afeta ambas as saídas de $a^{(3)}$, além de uma das saídas de $a^{(2)}$. Posto isto, a seguir os parâmetros são definidos em função das derivadas parciais conforme postulado pela regra da cadeia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(1)}_{11}} = \\frac{\\partial J(\\Theta)}{a_1^{(3)}} \\frac{\\partial a_1^{(3)}}{z_1^{(3)}} \\frac{\\partial z_1^{(3)}}{a^{(2)}_{1}} \\frac{\\partial a_1^{(2)}}{z_1^{(2)}} \\frac{\\partial z_1^{(2)}}{\\Theta^{(1)}_{11}} + \\frac{\\partial J(\\Theta)}{a_2^{(3)}} \\frac{\\partial a_2^{(3)}}{z_2^{(3)}} \\frac{\\partial z_2^{(3)}}{a^{(2)}_{1}} \\frac{\\partial a_1^{(2)}}{z_1^{(2)}} \\frac{\\partial z_1^{(2)}}{\\Theta^{(1)}_{11}}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(1)}_{12}} = \\frac{\\partial J(\\Theta)}{a_1^{(3)}} \\frac{\\partial a_1^{(3)}}{z_1^{(3)}} \\frac{\\partial z_1^{(3)}}{a^{(2)}_{1}} \\frac{\\partial a_1^{(2)}}{z_1^{(2)}} \\frac{\\partial z_1^{(2)}}{\\Theta^{(1)}_{12}} + \\frac{\\partial J(\\Theta)}{a_2^{(3)}} \\frac{\\partial a_2^{(3)}}{z_2^{(3)}} \\frac{\\partial z_2^{(3)}}{a^{(2)}_{1}} \\frac{\\partial a_1^{(2)}}{z_1^{(2)}} \\frac{\\partial z_1^{(2)}}{\\Theta^{(1)}_{12}}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(1)}_{21}} = \\frac{\\partial J(\\Theta)}{a_1^{(3)}} \\frac{\\partial a_1^{(3)}}{z_1^{(3)}} \\frac{\\partial z_1^{(3)}}{a^{(2)}_{2}} \\frac{\\partial a_2^{(2)}}{z_2^{(2)}} \\frac{\\partial z_2^{(2)}}{\\Theta^{(1)}_{21}} + \\frac{\\partial J(\\Theta)}{a_2^{(3)}} \\frac{\\partial a_2^{(3)}}{z_2^{(3)}} \\frac{\\partial z_2^{(3)}}{a^{(2)}_{2}} \\frac{\\partial a_2^{(2)}}{z_2^{(2)}} \\frac{\\partial z_2^{(2)}}{\\Theta^{(1)}_{21}}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(1)}_{22}} = \\frac{\\partial J(\\Theta)}{a_1^{(3)}} \\frac{\\partial a_1^{(3)}}{z_1^{(3)}} \\frac{\\partial z_1^{(3)}}{a^{(2)}_{2}} \\frac{\\partial a_2^{(2)}}{z_2^{(2)}} \\frac{\\partial z_2^{(2)}}{\\Theta^{(1)}_{22}} + \\frac{\\partial J(\\Theta)}{a_2^{(3)}} \\frac{\\partial a_2^{(3)}}{z_2^{(3)}} \\frac{\\partial z_2^{(3)}}{a^{(2)}_{2}} \\frac{\\partial a_2^{(2)}}{z_2^{(2)}} \\frac{\\partial z_2^{(2)}}{\\Theta^{(1)}_{22}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a análise dos parâmetros obtidos acima é possível constatar o alto custo computacional desta estratégia. Afinal mesmo para um modelo simples como o apresentado, com somente $8$ parâmetros, é necessária a solução de $52$ derivadas parciais.  \n",
    "\n",
    "Entretanto também é possível verificar que uma série destas derivadas se repetem. Sendo assim, a seguir é apresentado o algoritmo denominado de **Retropropagação**, utilizado para garantir a eficiência da etapa de treinamento das redes neurais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retropropagação\n",
    "\n",
    "A retropopagação (backpropagation) rescreve as derivadas parciais dos parâmetros do modelo para garantir a reutilização dos termos que se repetem. Iniciando pelos parâmetros da última camada, é possível verificar que as duas primeiras colunas se repetem e podem ser reescritas na forma de um termo $\\delta^{(3)}$, denominado de erro da última camada: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J(\\Theta)}{a_i^{(3)}} \\frac{\\partial a_i^{(3)}}{z_i^{(3)}} = \\delta_i^{(3)} = \\frac{-1}{m} (\\frac{y_i}{a_i^{3}} - \\frac{1 - y_i}{1 - a_i^{3}}) g'(a_i^{3})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescrevendo os parâmetros da última camada segundo a nova notação:\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(2)}_{11}} = \\delta_1^{(3)} \\frac{\\partial z_1^{(3)}}{\\Theta^{(2)}_{11}} = \\delta_1^{(3)} a_1^{(2)}$$\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(2)}_{12}} = \\delta_1^{(3)} \\frac{\\partial z_1^{(3)}}{\\Theta^{(2)}_{12}} = \\delta_1^{(3)} a_2^{(2)}$$\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(2)}_{21}} = \\delta_2^{(3)} \\frac{\\partial z_2^{(3)}}{\\Theta^{(2)}_{21}} = \\delta_2^{(3)} a_1^{(2)}$$\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(2)}_{22}} = \\delta_2^{(3)} \\frac{\\partial z_2^{(3)}}{\\Theta^{(2)}_{22}} = \\delta_2^{(3)} a_2^{(2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido a repetição dos termos, os parâmetros das demais camadas também podem ser reescritos com o objetivo de eliminar a redundância no cálculo das derivadas parciais. A seguir o parâmetro $\\Theta^{(1)}_{11}$ é redefinido, porém, o procedimento é idêntico para os demais parâmetros.\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(1)}_{11}} = \\frac{\\partial J(\\Theta)}{a_1^{(3)}} \\frac{\\partial a_1^{(3)}}{z_1^{(3)}} \\frac{\\partial z_1^{(3)}}{a^{(2)}_{1}} \\frac{\\partial a_1^{(2)}}{z_1^{(2)}} \\frac{\\partial z_1^{(2)}}{\\Theta^{(1)}_{11}} + \\frac{\\partial J(\\Theta)}{a_2^{(3)}} \\frac{\\partial a_2^{(3)}}{z_2^{(3)}} \\frac{\\partial z_2^{(3)}}{a^{(2)}_{1}} \\frac{\\partial a_1^{(2)}}{z_1^{(2)}} \\frac{\\partial z_1^{(2)}}{\\Theta^{(1)}_{11}}$$\n",
    "\n",
    "Primeiramente o erro da última camada é reutilizado:\n",
    "\n",
    "$$ \\frac{\\partial J(\\Theta)}{a_1^{(3)}} \\frac{\\partial a_1^{(3)}}{z_1^{(3)}} = \\delta_1^{(3)}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(1)}_{11}} = \\delta_1^{(3)} \\frac{\\partial z_1^{(3)}}{a^{(2)}_{1}} \\frac{\\partial a_1^{(2)}}{z_1^{(2)}} \\frac{\\partial z_1^{(2)}}{\\Theta^{(1)}_{11}} + \\delta_2^{(3)} \\frac{\\partial z_2^{(3)}}{a^{(2)}_{1}} \\frac{\\partial a_1^{(2)}}{z_1^{(2)}} \\frac{\\partial z_1^{(2)}}{\\Theta^{(1)}_{11}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível verificar também que $\\frac{\\partial z_1^{(3)}}{a^{(2)}_{1}} \\frac{\\partial a_1^{(2)}}{z_1^{(2)}} = \\Theta^{(2)}_{11} f'(a^{(2)})$ e $\\frac{\\partial z_2^{(3)}}{a^{(2)}_{1}} \\frac{\\partial a_1^{(2)}}{z_1^{(2)}} = \\Theta^{(2)}_{21} f'(a^{(2)})$, então:\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(1)}_{11}} = (\\delta_1^{(3)} \\Theta^{(2)}_{11} f'(a^{(2)}) + \\delta_2^{(3)} \\Theta^{(2)}_{21} f'(a^{(2)})) \\frac{\\partial z_1^{(2)}}{\\Theta^{(1)}_{11}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posto isto, podemos definir um novo termo referente ao erro, porém agora para as demais camadas:\n",
    "\n",
    "$$\\delta_1^{(2)} = \\delta_1^{(3)} \\Theta^{(2)}_{11} f'(a^{(2)}) + \\delta_2^{(3)} \\Theta^{(2)}_{21} f'(a^{(2)})$$\n",
    "\n",
    "$$\\delta_j^{(l)} = f'(a^{(l)}) \\sum\\limits_{i=1}^{n} \\delta_i^{(l+1)} \\Theta_{ij}^{(l)} $$\n",
    "\n",
    "Retomando o cálculo da derivada parcial dos parâmetros do modelo:\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(1)}_{11}} = \\delta_1^{(2)} \\frac{\\partial z_1^{(2)}}{\\Theta^{(1)}_{11}}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(1)}_{11}} = \\delta_1^{(2)} x_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfim, a derivada parcial dos parâmetros do modelo de uma rede neural podem ser definidos conforme segue:\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(l)}_{ij}} = (\\delta_j^{(l+1)})^T a^{(l)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo\n",
    "\n",
    "A seguir o algoritmo de retropropagação é resumido.\n",
    "\n",
    "1) Defina um conjunto de parâmetros inicial randômico e para cada exemplo de treinamento faça:\n",
    "\n",
    "2) Realize a propagação direta para obter as saídas do modelo.\n",
    "\n",
    "3) Calcule o erro para a última camada definida como $L$.\n",
    "\n",
    "$$\\delta_i^{(L)} = \\frac{-1}{m} (\\frac{y_i^{i}}{a_i^{L}} - \\frac{1 - y_i^{i}}{1 - a_i^{L}}) g'(a_i^{L})$$\n",
    "\n",
    "4) Defina a camada atual como a penúltima.\n",
    "\n",
    "$$l = L-1$$\n",
    "\n",
    "5) Acumule as derivadas parciais para a camada atual dado o exemplo de treinamento.\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\Theta^{(l)}_{ij}} \\mathrel{+}= (\\delta_j^{(l+1)})^T a^{(l)}$$\n",
    "\n",
    "6) Calcule o erro para a camada atual.\n",
    "\n",
    "$$\\delta_j^{(l)} = f'(a^{(l)}) \\sum\\limits_{i=1}^{n} \\delta_i^{(l+1)} \\Theta_{ij}^{(l)} $$\n",
    "\n",
    "7) Avance uma camada em direção ao início da rede.\n",
    "\n",
    "$$l = l-1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repita os passos de 5 a 7 até atingir a camada referente as entradas da rede. O script a seguir implementa o algoritmo e mostra a obtenção dos parâmetros para o operador XOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 7500\n",
      "[[0.9677338 ]\n",
      " [0.03452857]\n",
      " [0.03926295]\n",
      " [0.96096344]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def log_without_nan(x):\n",
    "    \"\"\"\n",
    "    Transforms nan to number\n",
    "    Arguments:\n",
    "        x: np.array (mx1)\n",
    "    Returns:\n",
    "        np.array (mx1)\n",
    "    \"\"\"\n",
    "    return np.nan_to_num(np.log(x))\n",
    "\n",
    "def logistic_regression_cost(h_x, y):\n",
    "    \"\"\"\n",
    "    Logistic regression cost\n",
    "    Arguments:\n",
    "        h_x: np.array (mx1)\n",
    "        y: np.array (mx1)\n",
    "    Returns:\n",
    "        double (half mse)\n",
    "    \"\"\"\n",
    "    m = np.shape(y)[0]\n",
    "    y_t = np.transpose(y) # mx1 -> 1xm\n",
    "\n",
    "    J = np.dot(-y_t, (log_without_nan(h_x))) - np.dot((1-y_t),(log_without_nan(1 - h_x))) # 1xn\n",
    "\n",
    "    return J/m\n",
    "\n",
    "def d_sigmoid(g_z):\n",
    "    \"\"\"\n",
    "    Derivate of sigmoid function\n",
    "    Arguments:\n",
    "        g_z: np.array (mx1)\n",
    "    Returns:\n",
    "        np.array (mx1)\n",
    "    \"\"\"\n",
    "    return g_z * (1 - g_z) \n",
    "    \n",
    "def backpropagation(x, o_list, y):\n",
    "    \"\"\"\n",
    "    Backpropagation\n",
    "    Arguments:\n",
    "        x: np.array (mxn)\n",
    "        o_list: list of np.array (rxs). \n",
    "                First should be r = n+1 and last s = k\n",
    "        y: np.array (mxk)\n",
    "    Returns:\n",
    "        list of np.array (rxs).\n",
    "    \"\"\"\n",
    "    m = np.shape(x)[0]\n",
    "    a = forward_propagation(x, o_list)\n",
    "    L = len(a) - 1\n",
    "\n",
    "    a_L = a[L] # output from model\n",
    "    delta_l = (-1/m) * (y/a_L - (1 - y)/(1-a_L)) * d_sigmoid(a_L)\n",
    "    \n",
    "    # backwards\n",
    "    dJ_o = []\n",
    "    l = L - 1\n",
    "    for o in reversed(o_list):\n",
    "        dJ_o.append(np.dot(np.transpose(a[l]), delta_l))\n",
    "        delta_l = np.dot(delta_l, np.transpose(o[1:])) * d_sigmoid(a[l]) # o[1:] to ignore bias\n",
    "        l = l - 1\n",
    "        \n",
    "    return list(reversed(dJ_o)) # reorder\n",
    "\n",
    "def gradient_descent(x, o_list, y, alpha, max_iterations, min_error):\n",
    "    \"\"\"\n",
    "    Gradient descent -> Discover what o minimize j\n",
    "    Arguments:\n",
    "        x: np.array (mxn)\n",
    "        o_list: list of np.array (rxs). \n",
    "                First should be r = n+1 and last s = k\n",
    "        y: np.array (mxk)\n",
    "        alpha: double (learning rate)\n",
    "        max_iterations: int\n",
    "        min_error: double (stop condition)\n",
    "    Returns:\n",
    "        o_list: list of np.array (rxs)\n",
    "        i: int (iterations number)\n",
    "        j_hist: list of double\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    grad = backpropagation(x, o_list, y)\n",
    "    j_hist = []\n",
    "    while np.linalg.norm(grad[-1]) > min_error and i < max_iterations:\n",
    "        for o_l, grad_l in zip(o_list, grad):\n",
    "            o_l[1:] = o_l[1:] - alpha*grad_l\n",
    "        \n",
    "        h_x = forward_propagation(x, o_list)[-1]\n",
    "        j_hist.append(logistic_regression_cost(h_x, y)[0])\n",
    "        \n",
    "        grad = backpropagation(x, o_list, y)\n",
    "        i += 1\n",
    "\n",
    "    return [o_list, i, j_hist]\n",
    "\n",
    "def neural_network_classifier(x, y, hidden_layers_sizes, alpha, max_iterations, min_error):\n",
    "    \"\"\"\n",
    "    Neural network classifier\n",
    "    Arguments:\n",
    "        x: np.array (mxn)\n",
    "        y: np.array (mxk)\n",
    "        hidden_layers_sizes: list of np.array (rxs)\n",
    "        alpha: double (learning rate)\n",
    "        max_iterations: int\n",
    "        min_error: double (stop condition)\n",
    "    Returns:\n",
    "        o_list: list of np.array (rxs)\n",
    "        i: int (iterations number)\n",
    "        j_hist: list of double\n",
    "    \"\"\"\n",
    "    \n",
    "    o_start_list = []\n",
    "    last_hidden_layer_size = np.shape(x)[1]\n",
    "    np.random.seed(SEED)\n",
    "    for hidden_layer_size in hidden_layers_sizes:\n",
    "        o_start_list.append(np.random.rand(last_hidden_layer_size + 1, hidden_layer_size))\n",
    "        last_hidden_layer_size = hidden_layer_size\n",
    "    \n",
    "    o_start_list.append(np.random.rand(last_hidden_layer_size + 1, np.shape(y)[1]))\n",
    "    return gradient_descent(x, o_start_list, y, alpha, max_iterations, min_error)\n",
    "\n",
    "# XOR expected output\n",
    "y = np.array([[1], [0], [0], [1]]) # 4x1\n",
    "\n",
    "alpha = 1e-1\n",
    "max_iterations = 30000 # 7500\n",
    "min_error = 1e-4\n",
    "hidden_layers_sizes = (2,) # (8,)\n",
    "[min_o, i, j_hist] = neural_network_classifier(x, y, hidden_layers_sizes, alpha, max_iterations, min_error)\n",
    "print('iterations:', i)\n",
    "print(forward_propagation(x, min_o)[-1])\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(j_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível verificar que o algoritmo atinge o mínimo da função custo, porém necessita de uma quantidade elevada de iterações do gradiente descendente. Também é possível constatar uma alta correlação entre o número de nodos da camada oculta e a quantidade de iterações.\n",
    "\n",
    "São alternativas para redução do custo computacional do treinamento a utilização de outros métodos de minimização, outros tipos de função de ativação, além da variação da quantidade de nodos das camadas ocultas. \n",
    "\n",
    "Posto isto, em seguida no material serão discutidas estratégias para seleção destes, comumente denominados de **hiperparâmetros**.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfim, um detalhamento maior do algoritmo de retropropagação, inclusive para outras funções custo, pode ser verificado em [Neural networks: training with backpropagation](https://www.jeremyjordan.me/neural-networks-training/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
