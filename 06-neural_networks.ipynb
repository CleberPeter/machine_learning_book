{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais\n",
    "\n",
    "As Redes Neurais Artificiais foram propostas inicialmente em meados de 1980 com o objetivo de imitar a forma como o cérebro humano aprende, especialmente no que se refere as conexões entre os neurônios. \n",
    "\n",
    "A ideia surgiu da observação de como partes do cerébro fisicamente vinculadas a um determinado sentido (audição, visão, tato) podem aprender a responder aos outros. Um dos experimentos realizados consistiu em redirecionar os sinais elétricos do nervo óptico (visão) para a região do cérebro responsável pela audição, o resultado surpreendente foi que, após algum tempo, a cobaia \"reaprendeu\" a enxergar. \n",
    "\n",
    "Com a adaptatividade do cérebro humano em mente o objetivo dos autores foi o de propor uma abordagem de aprendizado única e aplicável a qualquer tipo de problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivação\n",
    "\n",
    "A necessidade de um novo algoritmo para aprendizado de máquina surge de problemas não lineares com uma grande quantidade de entradas, $n > 100 $. Verificamos anteriormente que tanto a regressão linear quanto a logística podem ser adaptadas para mapearem superfícies não lineares, porém, para tanto é necessário adicionar novas entradas que representem a parcela quadrática, cúbica, ou de maior ordem conforme a superfície desejada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomando como exemplo os modelos linear e quadrático apresentados abaixo é possível verificar que a não linearidade aumenta substancialmente a quantidade de parâmetros do modelo.\n",
    "\n",
    "$$h_{\\theta}(x) = \\theta_{0} + \\theta_{1} x_1 + \\theta_{2} x_2$$\n",
    "$$h_{\\theta}(x) = \\theta_{0} + \\theta_{1} x_1 + \\theta_{2} x_2 + \\theta_{3} x_1 x_2 + \\theta_{4} x_1^2 + \\theta_{5} x_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a composição de um modelo não linear de um classificador de imagens com 50x50 pixels em escala de cinza ($n = 2500$) cada entrada deve ser combinada com as demais sem repetição, ou seja:\n",
    "\n",
    "$$h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1 x_2 + ... + \\theta x_1 x_{2500} + \\\\  \\theta x_2 + \\theta x_2^2 + \\theta x_2 x_3 + ... + \\theta x_2 x_{2500} + \\\\ ... + \\theta x_{2500} + \\theta x_{2500}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido a não repetição, $x_1$ realizará 2500 combinações, $x_2$ 2499, $x_3$ 2498 e assim por diante. Em outras palavras, a quantidade de parâmetros do modelo pode ser escrita na forma de uma soma de PA somada com os (n + 1) parâmetros já existentes no modelo linear:\n",
    "\n",
    "$$k = (2500 + 2499 + 2498 + ... + 3 + 2 + 1) + (1 + 2500)$$\n",
    "$$k = \\frac{(n+1) n}{2} + (n+1) = \\frac{(n+1) (n+ 2)}{2} = 3.128.751 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ademais, caso o classificador fosse projetado para processar imagens RGB ($n = 7500$) a quantidade de parâmetros necessária giraria em torno de 28 milhões. Enfim, a quantidade absurda de parâmetros, mesmo em um exemplo simples como o ilustrado, deixa clara a necessidade de uma nova estratégia para a composição de modelos não lineares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
