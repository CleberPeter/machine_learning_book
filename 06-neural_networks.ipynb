{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais\n",
    "\n",
    "As Redes Neurais Artificiais foram propostas inicialmente em meados de 1980 com o objetivo de imitar a forma como o cérebro humano aprende, especialmente no que se refere as conexões entre os neurônios. \n",
    "\n",
    "A ideia surgiu da observação de como partes do cerébro fisicamente vinculadas a um determinado sentido (audição, visão, tato) podem aprender a responder aos outros. Um dos experimentos realizados consistiu em redirecionar os sinais elétricos do nervo óptico (visão) para a região do cérebro responsável pela audição, o resultado surpreendente foi que, após algum tempo, a cobaia \"reaprendeu\" a enxergar. \n",
    "\n",
    "Com a adaptatividade do cérebro humano em mente o objetivo dos autores do modelo de rede neural foi o de propor uma abordagem de aprendizado aplicável a qualquer tipo de problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivação\n",
    "\n",
    "A necessidade de um novo algoritmo para aprendizado de máquina surge de problemas não lineares com uma grande quantidade de entradas, $n > 100 $. Verificamos anteriormente que tanto a regressão linear quanto a logística podem ser adaptadas para mapearem superfícies não lineares, porém, para tanto é necessário adicionar novas entradas que representem a parcela quadrática, cúbica, ou de maior ordem conforme a superfície desejada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomando como exemplo os modelos linear e quadrático apresentados abaixo é possível verificar que a não linearidade aumenta substancialmente a quantidade de parâmetros do modelo.\n",
    "\n",
    "$$h_{\\theta}(x) = \\theta_{0} + \\theta_{1} x_1 + \\theta_{2} x_2$$\n",
    "$$h_{\\theta}(x) = \\theta_{0} + \\theta_{1} x_1 + \\theta_{2} x_2 + \\theta_{3} x_1 x_2 + \\theta_{4} x_1^2 + \\theta_{5} x_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a composição de um modelo não linear de um classificador de imagens com 50x50 pixels em escala de cinza ($n = 2500$) cada entrada deve ser combinada com as demais sem repetição, ou seja:\n",
    "\n",
    "$$h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1 x_2 + ... + \\theta x_1 x_{2500} + \\\\  \\theta x_2 + \\theta x_2^2 + \\theta x_2 x_3 + ... + \\theta x_2 x_{2500} + \\\\ ... + \\theta x_{2500} + \\theta x_{2500}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido a não repetição, $x_1$ realizará 2500 combinações, $x_2$ 2499, $x_3$ 2498 e assim por diante. Em outras palavras, a quantidade de parâmetros do modelo pode ser escrita na forma de uma soma de PA somada com os (n + 1) parâmetros já existentes no modelo linear:\n",
    "\n",
    "$$k = (2500 + 2499 + 2498 + ... + 3 + 2 + 1) + (1 + 2500)$$\n",
    "$$k = \\frac{(n+1) n}{2} + (n+1) = \\frac{(n+1) (n+ 2)}{2} = 3.128.751 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ademais, caso o classificador fosse projetado para processar imagens RGB ($n = 7500$) a quantidade de parâmetros necessária giraria em torno de 28 milhões. Enfim, a quantidade absurda de parâmetros, mesmo em um exemplo simples como o ilustrado, deixa clara a necessidade de uma nova estratégia para a composição de modelos não lineares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As redes neurais são formadas por camadas, onde cada camada possui uma determinada quantidade de nodos. A primeira camada, denominada de **entrada**, recebe $x$ e possui portanto $n$ nodos. A última camada, denominada de **saída**, fornece $h_\\Theta(x)$ e possui uma quantidade de nodos igual a quantidade de saídas do modelo. As demais camadas são denominadas de **ocultas** e apresentam uma quantidade arbitrária de nodos. \n",
    "\n",
    "A título de exemplo, a Figura 1 ilustra uma rede neural com **2 nodos na camada de entrada**, **1 camada oculta com 2 nodos** e **uma camada de saída com somente 1 nodo**.\n",
    "\n",
    "<img src=\"imgs/neural_network.png\" alt=\"linear_model\" width=\"400\" style=\"display:block; margin:auto\"/>\n",
    "\n",
    "<div align=\"center\">\n",
    "Rede Neural.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os nodos $a^{(j)}_i$ são identificados conforme sua posição na rede, de tal forma que $j$ representa o número da camada e $i$ a posição do nodo dentro da camada. Sendo assim, na rede exemplificada pela Figura acima os nodo $a^{(2)}_1$ e $a^{(2)}_2$ representam o primeiro e o segundo nodo da camada oculta, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada nodo atua como uma unidade lógica de processamento que tem como saída a combinação linear das entradas $x$, ponderadas pelos parâmetros $\\Theta$ do modelo e aplicada em uma **função de ativação $g(z)$**. \n",
    "\n",
    "A função de ativação empregada varia conforme o tipo de problema, entretanto, quase sempre são utilizadas para refletir a característica não linear dos dados. São alguns exemplos de funções de ativação: a função simóide, a tangente hiperbólica e a ReLu.\n",
    "\n",
    "O modelo da camada oculta presente na Figura 1 é definido a seguir: \n",
    "\n",
    "$$a_1^{(2)} = g(\\Theta_{11}^{(1)} x_1 + \\Theta_{12}^{(1)} x_2)$$\n",
    "$$a_2^{(2)} = g(\\Theta_{21}^{(1)} x_1 + \\Theta_{22}^{(1)} x_2)$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} a_1^{(2)} \\\\ a_2^{(2)} \\end{bmatrix} =   \n",
    "g(\\begin{bmatrix} \\Theta_{11}^{(1)} & \\Theta_{12}^{(1)} \\\\\n",
    "                \\Theta_{21}^{(1)} & \\Theta_{22}^{(1)}\n",
    "\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix})\n",
    "= g(\\Theta^{(1)} x)\n",
    "$$\n",
    "\n",
    "Após a camada de entrada as demais utilizam como entrada a saída da camada imediatamente anterior. Posto isto, podemos escrever a saída do modelo:\n",
    "\n",
    "$$\n",
    "h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{11}^{(2)} a_1^{(2)} + \\Theta_{12}^{(2)} a_2^{(2)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_\\Theta(x) = \\begin{bmatrix} a_1^{(3)} \\end{bmatrix} =\n",
    "g(\\begin{bmatrix} \\Theta_{11}^{(2)} & \\Theta_{12}^{(2)}\n",
    "\\end{bmatrix} \\begin{bmatrix} a_1^{(2)} \\\\ a_2^{(2)} \\end{bmatrix})\n",
    "= g(\\Theta^{(2)} a^{(2)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viés (Bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em redes neurais é comum a adição de um nodo de **viés** igual a $1$ em cada uma das camadas que, diferentemente dos demais, não é conectado a camada anterior. Este nodo, representado na Figura 1 pelos nodos $x_0$ e $a_0^{(2)}$, tem como próposito fornecer maior expressividade ao modelo.\n",
    "\n",
    "<img src=\"imgs/neural_network_with_bias.png\" alt=\"linear_model\" width=\"400\" style=\"display:block; margin:auto\"/>\n",
    "\n",
    "<div align=\"center\">\n",
    "Figura 1 - Rede Neural com Viés.\n",
    "</div>\n",
    "\n",
    "O impacto do viés pode ser verificado na resposta de uma rede neural com somente um nodo e uma função de ativação linear, ilustrada pela Figura abaixo.\n",
    "\n",
    "<img src=\"imgs/bias_impact_on_neural_network.png\" alt=\"linear_model\" width=\"400\" style=\"display:block; margin:auto\"/>\n",
    "\n",
    "<div align=\"center\">\n",
    "Impacto do Bias na Rede Neural.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sem o termo de viés o modelo consegue mapear somente dados que passam pela origem do plano cartesiano, enquanto que com o termo o grau de flexibilidade do modelo se torna maior. Abaixo é definido o modelo da rede neural com a presença de viés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$a_1^{(2)} = g(\\Theta_{11}^{(1)} x_0 + \\Theta_{12}^{(1)} x_1 + \\Theta_{13}^{(1)} x_2) = g(\\Theta_{11}^{(1)} + \\Theta_{12}^{(1)} x_1 + \\Theta_{13}^{(1)} x_2)$$\n",
    "$$a_2^{(2)} = g(\\Theta_{21}^{(1)} x_0 + \\Theta_{22}^{(1)} x_1 + \\Theta_{23}^{(1)} x_2) = g(\\Theta_{21}^{(1)} + \\Theta_{22}^{(1)} x_1 + \\Theta_{23}^{(1)} x_2)$$\n",
    "$$\n",
    "\n",
    "\\begin{bmatrix} a_1^{(2)} \\\\ a_2^{(2)} \\end{bmatrix} =  \n",
    "g(\\begin{bmatrix} \\Theta_{11}^{(1)} & \\Theta_{12}^{(1)} & \\Theta_{13}^{(1)} \\\\\n",
    "                \\Theta_{21}^{(1)} & \\Theta_{22}^{(1)} & \\Theta_{23}^{(1)}\n",
    "\\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix})\n",
    "= g(\\Theta^{(1)} x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{11}^{(2)} a_0^{(2)} + \\Theta_{12}^{(2)} a_1^{(2)} + \\Theta_{13}^{(2)} a_2^{(2)}) = g(\\Theta_{11}^{(2)} + \\Theta_{12}^{(2)} a_1^{(2)} + \\Theta_{13}^{(2)} a_2^{(2)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_\\Theta(x) = \\begin{bmatrix} a_1^{(3)} \\end{bmatrix} =\n",
    "g(\\begin{bmatrix} \\Theta_{11}^{(2)} & \\Theta_{12}^{(2)} & \\Theta_{13}^{(2)}\n",
    "\\end{bmatrix} \\begin{bmatrix} a_0^{(2)} \\\\ a_1^{(2)} \\\\ a_2^{(2)} \\end{bmatrix})\n",
    "= g(\\Theta^{(2)} a^{(2)})$$\n",
    "\n",
    "$$a^j = g(\\Theta^{(j-1)} a^{(j-1)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada camada possui sua própria matriz de parâmetros os quais, no contexto das redes neurais, também são chamados de **pesos**. $\\Theta^{(j)}_{r x s}$ reflete os parâmetros da camada $j$ em que $r$ é dado pela quantidade de nodos da próxima camada e $s$ pela quantidade de nodos da camada atual $+1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a definição do modelo, uma observação se faz necessária. A última camada da rede neural possui o mesmo modelo da regressão logística, sendo assim, qual a vantagem deste tipo de abordagem ?\n",
    "\n",
    "A grande vantagem das redes neurais reside justamente na presença das camadas ocultas. O mapeamento de funções não lineares através da regressão logística era possível porém dependia da escolha adequada do polinômio, já para as redes neurais esse processo é automatizado pela fase de treinamento com a definição da matriz $\\Theta^1$.\n",
    "\n",
    "Ao invés de injetar as entradas diretamente no modelo de regressão logística, $\\Theta^1$ seleciona frações das entradas e as combina para permitir a composição de modelos altamente não lineares sem a necessidade de criação de novas entradas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo\n",
    "\n",
    "Uma forma de visualizar como modelos complexos podem ser obtidos com a adição de camadas ocultas é realizando a construção da rede de forma modular. \n",
    "\n",
    "A seguir é elaborada uma arquitetura de rede neural para classificação de dados binários segundo o operador lógico **XOR**, através da criação prévia dos operadores **AND**, **OR** e **(NOT $x_1$) AND (NOT $x_2$)**.\n",
    "\n",
    "<img src=\"imgs/xor_data_distribution.png\" alt=\"linear_model\" width=\"400\" style=\"display:block; margin:auto\"/>\n",
    "\n",
    "<div align=\"center\">\n",
    "Distribuição de Dados do Operador XOR.\n",
    "</div>\n",
    "\n",
    "### Operadores AND, OR e (NOT $x_1$) AND (NOT $x_2$)\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"imgs/ann_and.png\" alt=\"Drawing\"/> </td>\n",
    "<td> <img src=\"imgs/ann_or.png\" alt=\"Drawing\"/> </td>\n",
    "<td> <img src=\"imgs/ann_not_x1_and_not_x2.png\" alt=\"Drawing\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "<div align=\"center\">\n",
    "Redes Neurais dos Operadores Lógicos.\n",
    "</div>\n",
    "\n",
    "### Operador XOR\n",
    "\n",
    "<img src=\"imgs/ann_xor.png\" alt=\"linear_model\" width=\"600\" style=\"display:block; margin:auto\"/>\n",
    "\n",
    "<div align=\"center\">\n",
    "Rede Neural do Operador XOR.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme pode ser verificado na figura acima é possível a criação de um modelo altamente não linear com a adição de somente uma camada oculta. O modelo obtido é definido a seguir:\n",
    "\n",
    "$$XOR = h_\\Theta(x) = (x_1 . x_2) | (\\neg{x_1} . \\neg{x_2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
