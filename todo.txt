# Supervised Learning

    logistic regression : https://ricardomatsumura.medium.com/regress%C3%A3o-log%C3%ADstica-556d81684bf7
    support vector machines
        SVC, SVR
        C parameter is inverse of regularization parameter and is proportional to classifier margin
        in logistic regression (z > 0 , y = 1) in svm (z > 1, y = 1)
    decision tree, ramdom forest
    K-nearest neighbors

# Model Selection

    https://scikit-learn.org/stable/model_selection.html

    Overfitting
        split dataset -> train, validation and test workflow (shuffle) (validation_error is lower than test_error)
        cross validation, k-folds, bootstrapping, nested k fold cross validation

    Bias x Variance (validation curve)
        to chosing between models comparing modelsxerror in training and test:
        bias -> underfit, train and validation error are high
        variance -> overfit, train error low and validation error high 

    regularization -> minimize theta to smooth model -> final week 3, exercise 2
        bias -> high regularization parameter
        variance -> low regularization parameter
        different models emerge from different regularization parameters and variance and bias can be analyzed to chose
        in this curve high bias and high variance have the positions switched 

    learning curve
        varies the amount of training examples given to the model and monitors error in training and validation
        in this curve train start low and grows while the validation the opposite. that is, with few examples any model 
        adapts to the data (low training error), but with many examples the probability of generating a model that generalizes 
        is higher (low validation error).

        high bias -> the validation error does not decrease with examples increase, that is, the model does not improve with more data.
            get more features, increase model complexity 
        high variance -> there will be a gap between train and validation curves, decrease regularization parameter
            get more train examples, use less features, increase regularization parameter

unbalanced dataset
    undersampling, oversampling
    confusion matrix -> recall metric, fscore

hyperparameters tunning -> grid search, ramdom search

dimensionality reduction -> PCA, autoencoders

https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html

# Unsupervised Learning
    K-means

# Problems

    spam classifier